{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "spacy_linguistic.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yf6Jw7HwIec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veIGUtYKwIek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(\"I am looking forward to have a dinner with Saurav today.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4bs4E0gwIeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "59001391-0b9d-46db-989b-edb383155b3f"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text + ' - ' + token.dep_)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I - nsubj\n",
            "am - aux\n",
            "looking - ROOT\n",
            "forward - advmod\n",
            "to - aux\n",
            "have - xcomp\n",
            "a - det\n",
            "dinner - dobj\n",
            "with - prep\n",
            "Saurav - pobj\n",
            "today - npadvmod\n",
            ". - punct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYfK1iYWwIet",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "45770694-6739-4c6c-bcd4-154e8f6d3fb7"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "          [child for child in token.children])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I nsubj looking VERB []\n",
            "am aux looking VERB []\n",
            "looking ROOT looking VERB [I, am, forward, have, .]\n",
            "forward advmod looking VERB []\n",
            "to aux have VERB []\n",
            "have xcomp looking VERB [to, dinner, today]\n",
            "a det dinner NOUN []\n",
            "dinner dobj have VERB [a, with]\n",
            "with prep dinner NOUN [Saurav]\n",
            "Saurav pobj with ADP []\n",
            "today npadvmod have VERB []\n",
            ". punct looking VERB []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azxfUmY8wIex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ac83791-33c5-4363-c527-fd79b86e7893"
      },
      "source": [
        "import keras"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn3sWctCwIe0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95668588-c785-4427-98b1-16d1de705a72"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "'''An implementation of sequence to sequence learning for performing addition\n",
        "\n",
        "Input: \"535+61\"\n",
        "Output: \"596\"\n",
        "Padding is handled by using a repeated sentinel character (space)\n",
        "\n",
        "Input may optionally be reversed, shown to increase performance in many tasks in:\n",
        "\"Learning to Execute\"\n",
        "http://arxiv.org/abs/1410.4615\n",
        "and\n",
        "\"Sequence to Sequence Learning with Neural Networks\"\n",
        "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
        "Theoretically it introduces shorter term dependencies between source and target.\n",
        "\n",
        "Two digits reversed:\n",
        "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
        "\n",
        "Three digits reversed:\n",
        "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
        "\n",
        "Four digits reversed:\n",
        "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
        "\n",
        "Five digits reversed:\n",
        "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "from six.moves import range\n",
        "\n",
        "\n",
        "class CharacterTable(object):\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one hot integer representation\n",
        "    + Decode the one hot integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One hot encode given string C.\n",
        "\n",
        "        # Arguments\n",
        "            num_rows: Number of rows in the returned one hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "class colors:\n",
        "    ok = '\\033[92m'\n",
        "    fail = '\\033[91m'\n",
        "    close = '\\033[0m'\n",
        "\n",
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = True\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = '0123456789+ '\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print('Generating data...')\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
        "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = '{}+{}'.format(a, b)\n",
        "    query = q + ' ' * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print('Total addition questions:', len(questions))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "# digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print('Training Data:')\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print('Validation Data:')\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "\n",
        "# Try replacing GRU, or SimpleRNN.\n",
        "RNN = layers.LSTM\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "LAYERS = 1\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
        "# Note: In a situation where your input sequences have a variable length,\n",
        "# use input_shape=(None, num_feature).\n",
        "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
        "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
        "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "# The decoder RNN could be multiple layers stacked or a single layer.\n",
        "for _ in range(LAYERS):\n",
        "    # By setting return_sequences to True, return not only the last output but\n",
        "    # all the outputs so far in the form of (num_samples, timesteps,\n",
        "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
        "    # the first dimension to be the timesteps.\n",
        "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
        "\n",
        "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
        "# of the output sequence, decide which character should be chosen.\n",
        "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
        "model.add(layers.Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for iteration in range(1, 10):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration)\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=1,\n",
        "              validation_data=(x_val, y_val))\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = model.predict_classes(rowx, verbose=0)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
        "        print('T', correct, end=' ')\n",
        "        if correct == guess:\n",
        "            print(colors.ok + '☑' + colors.close, end=' ')\n",
        "        else:\n",
        "            print(colors.fail + '☒' + colors.close, end=' ')\n",
        "        print(guess)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating data...\n",
            "Total addition questions: 50000\n",
            "Vectorization...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0808 15:09:16.770908 140314601400192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0808 15:09:16.804280 140314601400192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0808 15:09:16.812062 140314601400192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Data:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "Validation Data:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n",
            "Build model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0808 15:09:17.355444 140314601400192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0808 15:09:17.386492 140314601400192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0808 15:09:17.569739 140314601400192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 128)               72192     \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 4, 12)             1548      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4, 12)             0         \n",
            "=================================================================\n",
            "Total params: 205,324\n",
            "Trainable params: 205,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0808 15:09:19.078428 140314601400192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 16s 354us/step - loss: 1.8856 - acc: 0.3223 - val_loss: 1.7940 - val_acc: 0.3455\n",
            "Q 958+1   T 959  \u001b[91m☒\u001b[0m 100 \n",
            "Q 4+945   T 949  \u001b[91m☒\u001b[0m 55  \n",
            "Q 557+727 T 1284 \u001b[91m☒\u001b[0m 105 \n",
            "Q 4+49    T 53   \u001b[91m☒\u001b[0m 55  \n",
            "Q 208+27  T 235  \u001b[91m☒\u001b[0m 126 \n",
            "Q 996+86  T 1082 \u001b[91m☒\u001b[0m 100 \n",
            "Q 52+869  T 921  \u001b[91m☒\u001b[0m 100 \n",
            "Q 2+929   T 931  \u001b[91m☒\u001b[0m 10  \n",
            "Q 13+525  T 538  \u001b[91m☒\u001b[0m 326 \n",
            "Q 610+9   T 619  \u001b[91m☒\u001b[0m 10  \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 2\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 14s 310us/step - loss: 1.7281 - acc: 0.3635 - val_loss: 1.6535 - val_acc: 0.3861\n",
            "Q 30+523  T 553  \u001b[91m☒\u001b[0m 348 \n",
            "Q 77+468  T 545  \u001b[91m☒\u001b[0m 788 \n",
            "Q 137+594 T 731  \u001b[91m☒\u001b[0m 101 \n",
            "Q 598+3   T 601  \u001b[91m☒\u001b[0m 982 \n",
            "Q 27+739  T 766  \u001b[91m☒\u001b[0m 288 \n",
            "Q 78+89   T 167  \u001b[91m☒\u001b[0m 188 \n",
            "Q 307+26  T 333  \u001b[91m☒\u001b[0m 288 \n",
            "Q 191+353 T 544  \u001b[91m☒\u001b[0m 101 \n",
            "Q 224+563 T 787  \u001b[91m☒\u001b[0m 101 \n",
            "Q 527+455 T 982  \u001b[91m☒\u001b[0m 101 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 3\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 14s 310us/step - loss: 1.5887 - acc: 0.4055 - val_loss: 1.5114 - val_acc: 0.4295\n",
            "Q 410+952 T 1362 \u001b[91m☒\u001b[0m 1422\n",
            "Q 48+79   T 127  \u001b[91m☒\u001b[0m 130 \n",
            "Q 32+415  T 447  \u001b[91m☒\u001b[0m 358 \n",
            "Q 904+8   T 912  \u001b[91m☒\u001b[0m 994 \n",
            "Q 84+739  T 823  \u001b[91m☒\u001b[0m 802 \n",
            "Q 8+231   T 239  \u001b[91m☒\u001b[0m 333 \n",
            "Q 87+328  T 415  \u001b[91m☒\u001b[0m 330 \n",
            "Q 670+8   T 678  \u001b[91m☒\u001b[0m 777 \n",
            "Q 712+785 T 1497 \u001b[91m☒\u001b[0m 1622\n",
            "Q 871+15  T 886  \u001b[91m☒\u001b[0m 882 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 4\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 14s 309us/step - loss: 1.4165 - acc: 0.4723 - val_loss: 1.3133 - val_acc: 0.5099\n",
            "Q 9+345   T 354  \u001b[91m☒\u001b[0m 446 \n",
            "Q 4+626   T 630  \u001b[91m☒\u001b[0m 661 \n",
            "Q 2+665   T 667  \u001b[91m☒\u001b[0m 666 \n",
            "Q 54+930  T 984  \u001b[91m☒\u001b[0m 901 \n",
            "Q 50+576  T 626  \u001b[91m☒\u001b[0m 655 \n",
            "Q 769+64  T 833  \u001b[91m☒\u001b[0m 751 \n",
            "Q 104+67  T 171  \u001b[91m☒\u001b[0m 211 \n",
            "Q 93+781  T 874  \u001b[91m☒\u001b[0m 851 \n",
            "Q 900+383 T 1283 \u001b[91m☒\u001b[0m 1289\n",
            "Q 814+72  T 886  \u001b[91m☒\u001b[0m 889 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 5\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 14s 307us/step - loss: 1.2535 - acc: 0.5366 - val_loss: 1.1968 - val_acc: 0.5512\n",
            "Q 567+6   T 573  \u001b[91m☒\u001b[0m 661 \n",
            "Q 360+57  T 417  \u001b[91m☒\u001b[0m 491 \n",
            "Q 499+65  T 564  \u001b[91m☒\u001b[0m 543 \n",
            "Q 977+50  T 1027 \u001b[91m☒\u001b[0m 1031\n",
            "Q 311+670 T 981  \u001b[91m☒\u001b[0m 1068\n",
            "Q 845+39  T 884  \u001b[91m☒\u001b[0m 891 \n",
            "Q 816+301 T 1117 \u001b[91m☒\u001b[0m 1166\n",
            "Q 54+893  T 947  \u001b[91m☒\u001b[0m 991 \n",
            "Q 40+755  T 795  \u001b[91m☒\u001b[0m 719 \n",
            "Q 53+33   T 86   \u001b[91m☒\u001b[0m 80  \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 6\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 14s 306us/step - loss: 1.1255 - acc: 0.5887 - val_loss: 1.0584 - val_acc: 0.6159\n",
            "Q 22+94   T 116  \u001b[91m☒\u001b[0m 127 \n",
            "Q 701+89  T 790  \u001b[91m☒\u001b[0m 798 \n",
            "Q 681+738 T 1419 \u001b[91m☒\u001b[0m 1499\n",
            "Q 120+801 T 921  \u001b[91m☒\u001b[0m 903 \n",
            "Q 780+1   T 781  \u001b[91m☒\u001b[0m 788 \n",
            "Q 4+83    T 87   \u001b[91m☒\u001b[0m 83  \n",
            "Q 475+36  T 511  \u001b[91m☒\u001b[0m 518 \n",
            "Q 87+21   T 108  \u001b[91m☒\u001b[0m 115 \n",
            "Q 306+9   T 315  \u001b[91m☒\u001b[0m 310 \n",
            "Q 95+975  T 1070 \u001b[91m☒\u001b[0m 1061\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 7\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 14s 306us/step - loss: 1.0099 - acc: 0.6340 - val_loss: 0.9563 - val_acc: 0.6560\n",
            "Q 99+62   T 161  \u001b[92m☑\u001b[0m 161 \n",
            "Q 134+680 T 814  \u001b[91m☒\u001b[0m 801 \n",
            "Q 940+266 T 1206 \u001b[91m☒\u001b[0m 1212\n",
            "Q 727+553 T 1280 \u001b[91m☒\u001b[0m 1278\n",
            "Q 510+149 T 659  \u001b[91m☒\u001b[0m 658 \n",
            "Q 89+847  T 936  \u001b[91m☒\u001b[0m 921 \n",
            "Q 753+890 T 1643 \u001b[91m☒\u001b[0m 1621\n",
            "Q 93+809  T 902  \u001b[91m☒\u001b[0m 801 \n",
            "Q 674+31  T 705  \u001b[91m☒\u001b[0m 706 \n",
            "Q 5+983   T 988  \u001b[91m☒\u001b[0m 992 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 8\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 14s 308us/step - loss: 0.9156 - acc: 0.6704 - val_loss: 0.8786 - val_acc: 0.6812\n",
            "Q 86+942  T 1028 \u001b[91m☒\u001b[0m 1026\n",
            "Q 13+845  T 858  \u001b[91m☒\u001b[0m 856 \n",
            "Q 849+99  T 948  \u001b[91m☒\u001b[0m 946 \n",
            "Q 78+51   T 129  \u001b[91m☒\u001b[0m 121 \n",
            "Q 88+361  T 449  \u001b[91m☒\u001b[0m 447 \n",
            "Q 123+303 T 426  \u001b[91m☒\u001b[0m 336 \n",
            "Q 239+206 T 445  \u001b[91m☒\u001b[0m 348 \n",
            "Q 9+925   T 934  \u001b[91m☒\u001b[0m 932 \n",
            "Q 713+451 T 1164 \u001b[91m☒\u001b[0m 1167\n",
            "Q 132+764 T 896  \u001b[92m☑\u001b[0m 896 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 9\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 14s 306us/step - loss: 0.8334 - acc: 0.7050 - val_loss: 0.8086 - val_acc: 0.7037\n",
            "Q 341+85  T 426  \u001b[91m☒\u001b[0m 418 \n",
            "Q 913+959 T 1872 \u001b[91m☒\u001b[0m 1877\n",
            "Q 36+381  T 417  \u001b[91m☒\u001b[0m 410 \n",
            "Q 527+977 T 1504 \u001b[91m☒\u001b[0m 1502\n",
            "Q 211+711 T 922  \u001b[91m☒\u001b[0m 923 \n",
            "Q 7+257   T 264  \u001b[91m☒\u001b[0m 262 \n",
            "Q 764+847 T 1611 \u001b[91m☒\u001b[0m 1619\n",
            "Q 579+27  T 606  \u001b[91m☒\u001b[0m 601 \n",
            "Q 37+71   T 108  \u001b[91m☒\u001b[0m 100 \n",
            "Q 83+161  T 244  \u001b[92m☑\u001b[0m 244 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYkJ0lgPwIe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fb41113-a68a-4e1d-aa7d-470b9212ead1"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "data = \"\"\" Jack and Jill went up the hill\\n\n",
        "\t\tTo fetch a pail of water\\n\n",
        "\t\tJack fell down and broke his crown\\n\n",
        "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0]\n",
        "print(encoded)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 2, 14, 15, 1, 16, 17, 18, 1, 3, 19, 20, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U0zb7l9wIe6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4a9c8d5-cc9b-4b1b-b739-c4d39a7f7d86"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnNu3XdswIe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "a2051d20-f01e-4b53-c457-221bb7ef2d46"
      },
      "source": [
        "sequences = list()\n",
        "for i in range(1, len(encoded)):\n",
        "    sequence = encoded[i - 1 : i + 1]\n",
        "    print(sequence)\n",
        "    sequences.append(sequence)\n",
        "\n",
        "print(len(sequences))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 1]\n",
            "[1, 3]\n",
            "[3, 4]\n",
            "[4, 5]\n",
            "[5, 6]\n",
            "[6, 7]\n",
            "[7, 8]\n",
            "[8, 9]\n",
            "[9, 10]\n",
            "[10, 11]\n",
            "[11, 12]\n",
            "[12, 13]\n",
            "[13, 2]\n",
            "[2, 14]\n",
            "[14, 15]\n",
            "[15, 1]\n",
            "[1, 16]\n",
            "[16, 17]\n",
            "[17, 18]\n",
            "[18, 1]\n",
            "[1, 3]\n",
            "[3, 19]\n",
            "[19, 20]\n",
            "[20, 21]\n",
            "24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sxidFUdwIfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = array(sequences)\n",
        "x, y = sequences[:, 0], sequences[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePyrMWj0wIfJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18fb4f72-4f6f-4c25-ea5c-13ec1aa707a3"
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print(y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 1. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 1. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]\n",
            "   [1. 0. 0. ... 0. 0. 0.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zz73FT6wIfN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "226767b1-3100-4442-b084-b8cfbb3ee033"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1, 10)             220       \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 50)                12200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 22)                1122      \n",
            "=================================================================\n",
            "Total params: 13,542\n",
            "Trainable params: 13,542\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y6uvffpwIfQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "1f866e25-f968-41d4-c0a6-95050f152520"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(x, y, epochs=500, verbose=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e4c71335d737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_1_input to have 2 dimensions, but got array with shape (50000, 7, 12)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9ERqdmTwIfT",
        "colab_type": "code",
        "colab": {},
        "outputId": "47ddc7ee-56d4-4a7a-cf51-d336bb512af8"
      },
      "source": [
        "in_text = 'water'\n",
        "print(in_text)\n",
        "encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "encoded = array(encoded)\n",
        "yhat = model.predict_classes(encoded, verbose=0)\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index == yhat:\n",
        "        print(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "water\n",
            "jack\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB14BjTcwIfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}